{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All imports needed for this Python notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request URL\n",
    "import requests\n",
    "import urllib\n",
    "# Web Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "# Managing Datasets\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "# Managing temporary files\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitelist Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold whitelist links while retrieving them\n",
    "whitelist_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "# Get first ten pages of Hypestat top sites ranking\n",
    "# Goes up to 2000\n",
    "max_page = 10\n",
    "\n",
    "for i in range(10):\n",
    "    page = i + 1\n",
    "    if i == 1:\n",
    "        page = ''\n",
    "    result = requests.get(f'https://hypestat.com/top-sites/{page}')\n",
    "    soup = BeautifulSoup(result.content, \"html.parser\")\n",
    "    rows = soup.find_all('dt')\n",
    "    rows = [r.find_all('a') for r in rows]\n",
    "    for row in rows:\n",
    "        link = row[len(row)-1].get('href')\n",
    "        link = link.split('https://hypestat.com/info/')[1]\n",
    "        whitelist_links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 50 sites from all countries through Alexa ranking\n",
    "country_codes = []\n",
    "\n",
    "result = requests.get(f'https://www.alexa.com/topsites/countries')\n",
    "soup = BeautifulSoup(result.content, \"html.parser\")\n",
    "spans = soup.find_all(\"ul\", {\"class\": \"span3\"})\n",
    "lists = [r.find_all('li') for r in spans]\n",
    "for list_ in lists:\n",
    "    for listitem in list_:\n",
    "        code = listitem.find('a').get('href')\n",
    "        country_codes.append(code)     \n",
    "\n",
    "for code in country_codes:\n",
    "    result = requests.get(f'https://www.alexa.com/topsites/{code}')\n",
    "    soup = BeautifulSoup(result.content, \"html.parser\")\n",
    "    divs = soup.find_all(\"div\", {\"class\": \"DescriptionCell\"})\n",
    "    for link_cell in divs:\n",
    "        link = link_cell.find('a').get('href')\n",
    "        link = link.split('/siteinfo/')[1]\n",
    "        whitelist_links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5398"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove whitespace and duplicates, print length of list\n",
    "whitelist_links = [link for link in whitelist_links if link]\n",
    "whitelist_links = list(set(whitelist_links))\n",
    "len(whitelist_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist_df = pd.DataFrame({'url':whitelist_links})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>finanzen.net</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quizizz.com</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vkmag.com</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ae.com</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>viva.co.id</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            url label\n",
       "0  finanzen.net  good\n",
       "1   quizizz.com  good\n",
       "2     vkmag.com  good\n",
       "3        ae.com  good\n",
       "4    viva.co.id  good"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitelist_df['label']='good'\n",
    "whitelist_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blacklist Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishtank_urls = requests.get('http://data.phishtank.com/data/online-valid.json')\n",
    "phishtank_urls = [u.get('url') for u in phishtank_urls.json()]\n",
    "blacklist_urls.extend(phishtank_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlhaus_urls = requests.get('https://urlhaus.abuse.ch/downloads/text/')\n",
    "urlhaus_urls = urlhaus_urls.text.split('\\r\\n')\n",
    "urlhaus_urls = [url for url in urlhaus_urls if not str(url).startswith('#')]\n",
    "blacklist_urls.extend(urlhaus_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1068571\n"
     ]
    }
   ],
   "source": [
    "# Remove whitespace and duplicates, print length of list\n",
    "blacklist_urls = [link for link in blacklist_urls if link]\n",
    "blacklist_urls = list(set(blacklist_urls))\n",
    "print(len(blacklist_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://185.244.25.239/OwO/Tsunami.x86</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://42.230.88.134:49922/Mozi.a</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://165.227.169.191/[x32]</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://117.242.210.128:46271/Mozi.m</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://182.116.65.218:39149/Mozi.m</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     url label\n",
       "0  http://185.244.25.239/OwO/Tsunami.x86   bad\n",
       "1      http://42.230.88.134:49922/Mozi.a   bad\n",
       "2           http://165.227.169.191/[x32]   bad\n",
       "3    http://117.242.210.128:46271/Mozi.m   bad\n",
       "4     http://182.116.65.218:39149/Mozi.m   bad"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blacklist_df = pd.DataFrame({'url':blacklist_urls})\n",
    "blacklist_df['label'] = 'bad'\n",
    "blacklist_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Mixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raw.githubusercontent.com/Hipo/university-domains-list/master/world_universities_and_domains.json\n",
    "\n",
    "https://raw.githubusercontent.com/faizann24/Using-machine-learning-to-detect-malicious-URLs/master/data/data2.csv\n",
    "\n",
    "https://raw.githubusercontent.com/faizann24/Using-machine-learning-to-detect-malicious-URLs/master/data/data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_urls = 'https://raw.githubusercontent.com/Hipo/university-domains-list/master/world_universities_and_domains.json'\n",
    "\n",
    "uni_domains = requests.get(uni_urls).content\n",
    "uni_domains = json.loads(uni_domains)\n",
    "uni_domains = [u.get('web_pages') for u in uni_domains]\n",
    "uni_domains = [domain for sublist in uni_domains for domain in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.marywood.edu</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.cstj.qc.ca</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://ccmt.cstj.qc.ca</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://ccml.cstj.qc.ca</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.lindenwood.edu/</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          url label\n",
       "0     http://www.marywood.edu  good\n",
       "1      https://www.cstj.qc.ca  good\n",
       "2     https://ccmt.cstj.qc.ca  good\n",
       "3     https://ccml.cstj.qc.ca  good\n",
       "4  http://www.lindenwood.edu/  good"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_df = pd.DataFrame({'url':uni_domains})\n",
    "external_df['label'] = 'good'\n",
    "external_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diaryofagameaddict.com</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>espdesign.com.au</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iamagameaddict.com</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kalantzis.net</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slightlyoffcenter.net</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      url label\n",
       "0  diaryofagameaddict.com   bad\n",
       "1        espdesign.com.au   bad\n",
       "2      iamagameaddict.com   bad\n",
       "3           kalantzis.net   bad\n",
       "4   slightlyoffcenter.net   bad"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/faizann24/Using-machine-learning-to-detect-malicious-URLs/master/data/data.csv'\n",
    "data1 = requests.get(url).text.split('\\n')\n",
    "with open('url_data.csv', 'w', encoding=\"utf-8\") as file:\n",
    "    for line in data1:\n",
    "        file.write(line)\n",
    "        file.write('\\n')\n",
    "        \n",
    "data1_df = pd.read_csv('url_data.csv', sep=',')\n",
    "#os.remove('url_data1.csv')\n",
    "data1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hottraveljobs.com/forum/docs/info.php</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news.grouptumbler.com/news/feed.php</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>info.leveldelta.com/php/text.php</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>citroen-club.ch/n.exe</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zehir4.asp</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     url label\n",
       "0  hottraveljobs.com/forum/docs/info.php   bad\n",
       "1    news.grouptumbler.com/news/feed.php   bad\n",
       "2       info.leveldelta.com/php/text.php   bad\n",
       "3                  citroen-club.ch/n.exe   bad\n",
       "4                             zehir4.asp   bad"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All labels in this dataset are bad\n",
    "url = 'https://raw.githubusercontent.com/faizann24/Using-machine-learning-to-detect-malicious-URLs/master/data/data2.csv'\n",
    "data2 = requests.get(url).text.split('\\n')\n",
    "with open('url_data2.csv', 'w', encoding=\"utf-8\") as file:\n",
    "    for line in data2:\n",
    "        file.write(line)\n",
    "        file.write('\\n')\n",
    "        \n",
    "data2_df = pd.read_csv('url_data2.csv', sep=',', header=None, names=['url', 'label'])\n",
    "#os.remove('url_data2.csv')\n",
    "data2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 463075 entries, 0 to 32875\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   url     463075 non-null  object\n",
      " 1   label   463075 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 10.6+ MB\n"
     ]
    }
   ],
   "source": [
    "external_df = pd.concat([external_df, data1_df, data2_df])\n",
    "external_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_scheme(row):\n",
    "    if row['url'].startswith('http'):\n",
    "        return row['url']\n",
    "    \n",
    "    domain = row['url']\n",
    "    url = f'https://www.{domain}'\n",
    "    try:\n",
    "        requests.get(url, timeout = 5)\n",
    "        return url\n",
    "    except:\n",
    "        return f'http://www.{domain}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           url\n",
      "label         \n",
      "bad    1135019\n",
      "good    359886\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1537044 entries, 0 to 32875\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   url     1537044 non-null  object\n",
      " 1   label   1537044 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 35.2+ MB\n"
     ]
    }
   ],
   "source": [
    "url_df = pd.concat([blacklist_df, whitelist_df, external_df])\n",
    "print(url_df.groupby('label').nunique())\n",
    "url_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359954"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minority_count = len(url_df.loc[url_df['label'] == 'good'])\n",
    "minority_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://185.244.25.239/OwO/Tsunami.x86</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://42.230.88.134:49922/Mozi.a</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://165.227.169.191/[x32]</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://117.242.210.128:46271/Mozi.m</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://182.116.65.218:39149/Mozi.m</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     url label\n",
       "0  http://185.244.25.239/OwO/Tsunami.x86   bad\n",
       "1      http://42.230.88.134:49922/Mozi.a   bad\n",
       "2           http://165.227.169.191/[x32]   bad\n",
       "3    http://117.242.210.128:46271/Mozi.m   bad\n",
       "4     http://182.116.65.218:39149/Mozi.m   bad"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_df.duplicated(subset=['url'])\n",
    "url_df.drop_duplicates(subset=['url'])\n",
    "url_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we reduce and balance the dataset because of two reasons. Firstly, the dataset is quite imbalanced and secondly there is too much data and processing it takes too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          url\n",
      "label        \n",
      "bad    355208\n",
      "good   359886\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 719908 entries, 156712 to 166477\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   url     719908 non-null  object\n",
      " 1   label   719908 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 16.5+ MB\n"
     ]
    }
   ],
   "source": [
    "good_df = url_df[url_df.label == \"good\"].sample(minority_count)\n",
    "bad_df = url_df[url_df.label == \"bad\"].sample(minority_count)\n",
    "url_df = pd.concat([good_df, bad_df])\n",
    "print(url_df.groupby('label').nunique())\n",
    "url_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df['url'] = url_df.apply(lambda row: domain_scheme(row), axis=1)\n",
    "url_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df.to_csv('raw_data/urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
